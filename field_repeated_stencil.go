// Copyright 2020-2025 Buf Technologies, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package fastpb

// Code generated by internal/stencil. DO NOT EDIT

import (
	"math/bits"

	"github.com/bufbuild/fastpb/internal/arena"
	"github.com/bufbuild/fastpb/internal/dbg"
	"github.com/bufbuild/fastpb/internal/unsafe2"
)

//go:nosplit
func spillArena8(p1 parser1, p2 parser2, rep arena.Slice[uint8]) (parser1, parser2, arena.Slice[uint8]) {
	_ = spillArena[uint8]
	return p1, p2, arena.SliceOf(p1.arena(), unwrapZC(rep, p1.c().src)...)
}

//go:nosplit
func spillArena32(p1 parser1, p2 parser2, rep arena.Slice[uint32]) (parser1, parser2, arena.Slice[uint32]) {
	_ = spillArena[uint32]
	return p1, p2, arena.SliceOf(p1.arena(), unwrapZC(rep, p1.c().src)...)
}

//go:nosplit
func spillArena64(p1 parser1, p2 parser2, rep arena.Slice[uint64]) (parser1, parser2, arena.Slice[uint64]) {
	_ = spillArena[uint64]
	return p1, p2, arena.SliceOf(p1.arena(), unwrapZC(rep, p1.c().src)...)
}

//go:nosplit
func parseRepeatedVarint8(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parseRepeatedVarint[uint8]
	var n uint64
	p1, p2, n = p1.varint(p2)

	slot := unsafe2.Cast[arena.SliceAddr[uint8]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	if isZC(slice) && slice.Cap() > 0 {

		zc := unwrapRawZC(slice).bytes(p1.c().src)
		slice := arena.NewSlice[uint8](p1.arena(), len(zc)+1)
		for i, b := range zc {
			slice.Store(i, uint8(b))
		}
		slice.Store(slice.Len()-1, uint8(n))
		p1.log(p2, "spill", "%v %v", slice.Addr(), slice)

		*slot = slice.Addr()
		return p1, p2
	}

	if slice.Len() < slice.Cap() {
		slice = slice.SetLen(slice.Len() + 1)
		slice.Store(slice.Len()-1, uint8(n))

		p1.log(p2, "store", "%v %v", slice.Addr(), slice)
		*slot = slice.Addr()
		return p1, p2
	}

	slice = slice.AppendOne(p1.arena(), uint8(n))
	p1.log(p2, "append", "%v %v", slice.Addr(), slice)
	*slot = slice.Addr()
	return p1, p2
}

//go:nosplit
func parseRepeatedVarint32(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parseRepeatedVarint[uint32]
	var n uint64
	p1, p2, n = p1.varint(p2)

	slot := unsafe2.Cast[arena.SliceAddr[uint32]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	if isZC(slice) && slice.Cap() > 0 {

		zc := unwrapRawZC(slice).bytes(p1.c().src)
		slice := arena.NewSlice[uint32](p1.arena(), len(zc)+1)
		for i, b := range zc {
			slice.Store(i, uint32(b))
		}
		slice.Store(slice.Len()-1, uint32(n))
		p1.log(p2, "spill", "%v %v", slice.Addr(), slice)

		*slot = slice.Addr()
		return p1, p2
	}

	if slice.Len() < slice.Cap() {
		slice = slice.SetLen(slice.Len() + 1)
		slice.Store(slice.Len()-1, uint32(n))

		p1.log(p2, "store", "%v %v", slice.Addr(), slice)
		*slot = slice.Addr()
		return p1, p2
	}

	slice = slice.AppendOne(p1.arena(), uint32(n))
	p1.log(p2, "append", "%v %v", slice.Addr(), slice)
	*slot = slice.Addr()
	return p1, p2
}

//go:nosplit
func parseRepeatedVarint64(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parseRepeatedVarint[uint64]
	var n uint64
	p1, p2, n = p1.varint(p2)

	slot := unsafe2.Cast[arena.SliceAddr[uint64]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	if isZC(slice) && slice.Cap() > 0 {

		zc := unwrapRawZC(slice).bytes(p1.c().src)
		slice := arena.NewSlice[uint64](p1.arena(), len(zc)+1)
		for i, b := range zc {
			slice.Store(i, uint64(b))
		}
		slice.Store(slice.Len()-1, uint64(n))
		p1.log(p2, "spill", "%v %v", slice.Addr(), slice)

		*slot = slice.Addr()
		return p1, p2
	}

	if slice.Len() < slice.Cap() {
		slice = slice.SetLen(slice.Len() + 1)
		slice.Store(slice.Len()-1, uint64(n))

		p1.log(p2, "store", "%v %v", slice.Addr(), slice)
		*slot = slice.Addr()
		return p1, p2
	}

	slice = slice.AppendOne(p1.arena(), uint64(n))
	p1.log(p2, "append", "%v %v", slice.Addr(), slice)
	*slot = slice.Addr()
	return p1, p2
}

//go:nosplit
func parsePackedVarint8(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parsePackedVarint[uint8]
	var n uint32
	p1, p2, n = p1.lengthPrefix(p2)
	if n == 0 {
		return p1, p2
	}

	p2.scratch = uint64(p1.e_)
	p1.e_ = p1.b_.Add(int(n))

	var count int
	for p := p1.b_; p < p1.e_; p += 8 {
		n := min(8, p1.e_-p)
		bytes := *unsafe2.Cast[uint64](p.AssertValid())
		bytes |= signBits << (n * 8)
		count += bits.OnesCount64(signBits &^ bytes)
	}

	slot := unsafe2.Cast[arena.SliceAddr[uint8]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()
	if isZC(slice) {

		switch {
		case slice.Cap() > 0:

			zc := unwrapRawZC(slice).bytes(p1.c().src)
			slice = arena.NewSlice[uint8](p1.arena(), len(zc)+count)
			for i, b := range zc {
				slice.Store(i, uint8(b))
			}
			slice = slice.SetLen(len(zc))

			p1.log(p2, "spill", "%v %v", slice.Addr(), slice)

		case count == int(n):
			*slot = wrapZC[uint8](zc{
				offset:	uint32(p1.b_.Sub(unsafe2.AddrOf(p1.c().src))),
				len:	n,
			}).Addr()

			if dbg.Enabled {
				raw := unwrapRawZC(slot.AssertValid()).bytes(p1.c().src)
				p1.log(p2, "zc", "%v %v", *slot, raw)
			}

			p1.b_ = p1.e_
			p1.e_ = unsafe2.Addr[byte](p2.scratch)
			return p1, p2

		default:
			slice = slice.Grow(p1.arena(), count)
			p1.log(p2, "grow", "%v %v", slice.Addr(), slice)
		}
	} else if spare := slice.Cap() - slice.Len(); spare < count {
		slice = slice.Grow(p1.arena(), count-spare)
		p1.log(p2, "grow", "%v %v, %d", slice.Addr(), slice, spare)
	}

	p := unsafe2.AddrOf(slice.Ptr()).Add(slice.Len())

	switch {
	case count == p1.len():
		for {
			*p.AssertValid() = uint8(*p1.b())
			p1.b_++
			p = p.Add(1)

			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	case count >= p1.len()/2:
		for {
			var x uint64
			if v := *p1.b(); int8(v) >= 0 {
				x = uint64(v)
				p1.b_++
			} else if c := p1.b_.Add(1); c != p1.e_ && int8(*c.AssertValid()) >= 0 {
				x = uint64(*p1.b()&0x7f) | uint64(*c.AssertValid())<<7
				p1.b_ += 2
			} else {
				p1, p2, x = p1.varint(p2)
			}

			*p.AssertValid() = uint8(x)
			p = p.Add(1)
			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	default:
		for {
			var x uint64
			p1, p2, x = p1.varint(p2)

			*p.AssertValid() = uint8(x)
			p = p.Add(1)
			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	}

	slice = slice.SetLen(p.Sub(unsafe2.AddrOf(slice.Ptr())))
	p1.log(p2, "append", "%v %v", slice.Addr(), slice)

	*slot = slice.Addr()
	p1.e_ = unsafe2.Addr[byte](p2.scratch)
	return p1, p2
}

//go:nosplit
func parsePackedVarint32(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parsePackedVarint[uint32]
	var n uint32
	p1, p2, n = p1.lengthPrefix(p2)
	if n == 0 {
		return p1, p2
	}

	p2.scratch = uint64(p1.e_)
	p1.e_ = p1.b_.Add(int(n))

	var count int
	for p := p1.b_; p < p1.e_; p += 8 {
		n := min(8, p1.e_-p)
		bytes := *unsafe2.Cast[uint64](p.AssertValid())
		bytes |= signBits << (n * 8)
		count += bits.OnesCount64(signBits &^ bytes)
	}

	slot := unsafe2.Cast[arena.SliceAddr[uint32]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()
	if isZC(slice) {

		switch {
		case slice.Cap() > 0:

			zc := unwrapRawZC(slice).bytes(p1.c().src)
			slice = arena.NewSlice[uint32](p1.arena(), len(zc)+count)
			for i, b := range zc {
				slice.Store(i, uint32(b))
			}
			slice = slice.SetLen(len(zc))

			p1.log(p2, "spill", "%v %v", slice.Addr(), slice)

		case count == int(n):
			*slot = wrapZC[uint32](zc{
				offset:	uint32(p1.b_.Sub(unsafe2.AddrOf(p1.c().src))),
				len:	n,
			}).Addr()

			if dbg.Enabled {
				raw := unwrapRawZC(slot.AssertValid()).bytes(p1.c().src)
				p1.log(p2, "zc", "%v %v", *slot, raw)
			}

			p1.b_ = p1.e_
			p1.e_ = unsafe2.Addr[byte](p2.scratch)
			return p1, p2

		default:
			slice = slice.Grow(p1.arena(), count)
			p1.log(p2, "grow", "%v %v", slice.Addr(), slice)
		}
	} else if spare := slice.Cap() - slice.Len(); spare < count {
		slice = slice.Grow(p1.arena(), count-spare)
		p1.log(p2, "grow", "%v %v, %d", slice.Addr(), slice, spare)
	}

	p := unsafe2.AddrOf(slice.Ptr()).Add(slice.Len())

	switch {
	case count == p1.len():
		for {
			*p.AssertValid() = uint32(*p1.b())
			p1.b_++
			p = p.Add(1)

			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	case count >= p1.len()/2:
		for {
			var x uint64
			if v := *p1.b(); int8(v) >= 0 {
				x = uint64(v)
				p1.b_++
			} else if c := p1.b_.Add(1); c != p1.e_ && int8(*c.AssertValid()) >= 0 {
				x = uint64(*p1.b()&0x7f) | uint64(*c.AssertValid())<<7
				p1.b_ += 2
			} else {
				p1, p2, x = p1.varint(p2)
			}

			*p.AssertValid() = uint32(x)
			p = p.Add(1)
			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	default:
		for {
			var x uint64
			p1, p2, x = p1.varint(p2)

			*p.AssertValid() = uint32(x)
			p = p.Add(1)
			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	}

	slice = slice.SetLen(p.Sub(unsafe2.AddrOf(slice.Ptr())))
	p1.log(p2, "append", "%v %v", slice.Addr(), slice)

	*slot = slice.Addr()
	p1.e_ = unsafe2.Addr[byte](p2.scratch)
	return p1, p2
}

//go:nosplit
func parsePackedVarint64(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parsePackedVarint[uint64]
	var n uint32
	p1, p2, n = p1.lengthPrefix(p2)
	if n == 0 {
		return p1, p2
	}

	p2.scratch = uint64(p1.e_)
	p1.e_ = p1.b_.Add(int(n))

	var count int
	for p := p1.b_; p < p1.e_; p += 8 {
		n := min(8, p1.e_-p)
		bytes := *unsafe2.Cast[uint64](p.AssertValid())
		bytes |= signBits << (n * 8)
		count += bits.OnesCount64(signBits &^ bytes)
	}

	slot := unsafe2.Cast[arena.SliceAddr[uint64]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()
	if isZC(slice) {

		switch {
		case slice.Cap() > 0:

			zc := unwrapRawZC(slice).bytes(p1.c().src)
			slice = arena.NewSlice[uint64](p1.arena(), len(zc)+count)
			for i, b := range zc {
				slice.Store(i, uint64(b))
			}
			slice = slice.SetLen(len(zc))

			p1.log(p2, "spill", "%v %v", slice.Addr(), slice)

		case count == int(n):
			*slot = wrapZC[uint64](zc{
				offset:	uint32(p1.b_.Sub(unsafe2.AddrOf(p1.c().src))),
				len:	n,
			}).Addr()

			if dbg.Enabled {
				raw := unwrapRawZC(slot.AssertValid()).bytes(p1.c().src)
				p1.log(p2, "zc", "%v %v", *slot, raw)
			}

			p1.b_ = p1.e_
			p1.e_ = unsafe2.Addr[byte](p2.scratch)
			return p1, p2

		default:
			slice = slice.Grow(p1.arena(), count)
			p1.log(p2, "grow", "%v %v", slice.Addr(), slice)
		}
	} else if spare := slice.Cap() - slice.Len(); spare < count {
		slice = slice.Grow(p1.arena(), count-spare)
		p1.log(p2, "grow", "%v %v, %d", slice.Addr(), slice, spare)
	}

	p := unsafe2.AddrOf(slice.Ptr()).Add(slice.Len())

	switch {
	case count == p1.len():
		for {
			*p.AssertValid() = uint64(*p1.b())
			p1.b_++
			p = p.Add(1)

			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	case count >= p1.len()/2:
		for {
			var x uint64
			if v := *p1.b(); int8(v) >= 0 {
				x = uint64(v)
				p1.b_++
			} else if c := p1.b_.Add(1); c != p1.e_ && int8(*c.AssertValid()) >= 0 {
				x = uint64(*p1.b()&0x7f) | uint64(*c.AssertValid())<<7
				p1.b_ += 2
			} else {
				p1, p2, x = p1.varint(p2)
			}

			*p.AssertValid() = uint64(x)
			p = p.Add(1)
			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	default:
		for {
			var x uint64
			p1, p2, x = p1.varint(p2)

			*p.AssertValid() = uint64(x)
			p = p.Add(1)
			if p1.b_ != p1.e_ {
				continue
			}

			break
		}
	}

	slice = slice.SetLen(p.Sub(unsafe2.AddrOf(slice.Ptr())))
	p1.log(p2, "append", "%v %v", slice.Addr(), slice)

	*slot = slice.Addr()
	p1.e_ = unsafe2.Addr[byte](p2.scratch)
	return p1, p2
}

//go:nosplit
func appendFixed32(p1 parser1, p2 parser2, v uint32) (parser1, parser2) {
	_ = appendFixed[uint32]
	slot := unsafe2.Cast[arena.SliceAddr[uint32]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	if isZC(slice) && slice.Cap() > 0 {

		p1, p2, slice = spillArena32(p1, p2, slice)
		p1.log(p2, "repeated fixed spill", "%v %v", slice.Addr(), slice)
	}

	if slice.Len() < slice.Cap() {
		slice = slice.SetLen(slice.Len() + 1)
		slice.Store(slice.Len()-1, v)
		p1.log(p2, "repeated fixed store", "%v %v", slice.Addr(), slice)

		*slot = slice.Addr()
		return p1, p2
	}

	slice = slice.AppendOne(p1.arena(), v)
	p1.log(p2, "repeated fixed append", "%v %v", slice.Addr(), slice)
	*slot = slice.Addr()
	return p1, p2
}

//go:nosplit
func appendFixed64(p1 parser1, p2 parser2, v uint64) (parser1, parser2) {
	_ = appendFixed[uint64]
	slot := unsafe2.Cast[arena.SliceAddr[uint64]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	if isZC(slice) && slice.Cap() > 0 {

		p1, p2, slice = spillArena64(p1, p2, slice)
		p1.log(p2, "repeated fixed spill", "%v %v", slice.Addr(), slice)
	}

	if slice.Len() < slice.Cap() {
		slice = slice.SetLen(slice.Len() + 1)
		slice.Store(slice.Len()-1, v)
		p1.log(p2, "repeated fixed store", "%v %v", slice.Addr(), slice)

		*slot = slice.Addr()
		return p1, p2
	}

	slice = slice.AppendOne(p1.arena(), v)
	p1.log(p2, "repeated fixed append", "%v %v", slice.Addr(), slice)
	*slot = slice.Addr()
	return p1, p2
}

//go:nosplit
func parsePackedFixed32(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parsePackedFixed[uint32]
	var zc zc
	p1, p2, zc = p1.bytes(p2)
	if zc.len == 0 {
		return p1, p2
	}

	size, _ := unsafe2.Layout[uint32]()
	if int(zc.len)%size != 0 {
		p1.fail(p2, errCodeTruncated)
	}

	slot := unsafe2.Cast[arena.SliceAddr[uint32]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	switch {
	case !isZC(slice):

	case slice.Cap() == 0:

		*slot = wrapZC[uint32](zc).Addr()
		goto exit
	default:

		p1, p2, slice = spillArena(p1, p2, slice)
	}

	{
		size, _ := unsafe2.Layout[uint32]()
		borrowed := unsafe2.Slice(
			unsafe2.Cast[uint32](unsafe2.Add(p1.c().src, zc.offset)),
			int(zc.len)/size,
		)

		*slot = slice.Append(p1.arena(), borrowed...).Addr()
	}

exit:
	return p1, p2
}

//go:nosplit
func parsePackedFixed64(p1 parser1, p2 parser2) (parser1, parser2) {
	_ = parsePackedFixed[uint64]
	var zc zc
	p1, p2, zc = p1.bytes(p2)
	if zc.len == 0 {
		return p1, p2
	}

	size, _ := unsafe2.Layout[uint64]()
	if int(zc.len)%size != 0 {
		p1.fail(p2, errCodeTruncated)
	}

	slot := unsafe2.Cast[arena.SliceAddr[uint64]](unsafe2.ByteAdd(p2.m(), p2.f().offset.data))
	slice := slot.AssertValid()

	switch {
	case !isZC(slice):

	case slice.Cap() == 0:

		*slot = wrapZC[uint64](zc).Addr()
		goto exit
	default:

		p1, p2, slice = spillArena(p1, p2, slice)
	}

	{
		size, _ := unsafe2.Layout[uint64]()
		borrowed := unsafe2.Slice(
			unsafe2.Cast[uint64](unsafe2.Add(p1.c().src, zc.offset)),
			int(zc.len)/size,
		)

		*slot = slice.Append(p1.arena(), borrowed...).Addr()
	}

exit:
	return p1, p2
}
